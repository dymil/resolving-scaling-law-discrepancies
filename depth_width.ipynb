{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import *\n",
    "import pandas as pd\n",
    "from data import *\n",
    "from analysis_parametric import *\n",
    "from paper_figures import *\n",
    "from paper_tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/experiment_results.pickle.xz', compression='xz')\n",
    "df = process_big_df(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sweep = process_sweep_df(df.query(\"hparams=='sweep'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipliers = np.linspace(2,20,90)\n",
    "df_sweep_mults = process_sweep_df(df.query(\"hparams=='sweep'\"), trunc=multipliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like 67% sure we don't need to interpolate (because that's for finding minima), so straight to trying to fit power laws to the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>hparams</th>\n",
       "      <th>warmup</th>\n",
       "      <th>decay</th>\n",
       "      <th>width</th>\n",
       "      <th>depth</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/loss_std</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>train/batch_time</th>\n",
       "      <th>...</th>\n",
       "      <th>params_all</th>\n",
       "      <th>flops_per_token_att_no_embed</th>\n",
       "      <th>flops_per_token_att</th>\n",
       "      <th>flops_per_token_cc</th>\n",
       "      <th>flops_per_token_no_att</th>\n",
       "      <th>flops_per_token_no_att_no_embed</th>\n",
       "      <th>flops_per_token</th>\n",
       "      <th>params</th>\n",
       "      <th>eff_params_att</th>\n",
       "      <th>train/loss_smoothed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rw</td>\n",
       "      <td>base</td>\n",
       "      <td>short</td>\n",
       "      <td>kaplan</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>step\n",
       "26       8.273299\n",
       "52       7.264460\n",
       "104  ...</td>\n",
       "      <td>step\n",
       "26       0.001030\n",
       "52       0.001519\n",
       "104  ...</td>\n",
       "      <td>step\n",
       "20       9.578757\n",
       "40       8.079118\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       1.779191\n",
       "40       1.361588\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>672079872.0</td>\n",
       "      <td>3.693920e+09</td>\n",
       "      <td>4.090921e+09</td>\n",
       "      <td>4.459618e+09</td>\n",
       "      <td>3.671753e+09</td>\n",
       "      <td>3.274752e+09</td>\n",
       "      <td>3.671753e+09</td>\n",
       "      <td>611958784.0</td>\n",
       "      <td>681820160.0</td>\n",
       "      <td>10.0       9.578757\n",
       "30.0       8.079118\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rw</td>\n",
       "      <td>base</td>\n",
       "      <td>short</td>\n",
       "      <td>kaplan</td>\n",
       "      <td>832.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>step\n",
       "36       7.209037\n",
       "72       6.616290\n",
       "144  ...</td>\n",
       "      <td>step\n",
       "36       0.001431\n",
       "72       0.001538\n",
       "144  ...</td>\n",
       "      <td>step\n",
       "20       8.892697\n",
       "40       7.436884\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.682528\n",
       "40       0.471070\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>260063232.0</td>\n",
       "      <td>1.288176e+09</td>\n",
       "      <td>1.539932e+09</td>\n",
       "      <td>1.729344e+09</td>\n",
       "      <td>1.325236e+09</td>\n",
       "      <td>1.073480e+09</td>\n",
       "      <td>1.325236e+09</td>\n",
       "      <td>220872704.0</td>\n",
       "      <td>256655360.0</td>\n",
       "      <td>10.0       8.892697\n",
       "30.0       7.436884\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rw</td>\n",
       "      <td>base</td>\n",
       "      <td>short</td>\n",
       "      <td>kaplan</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>step\n",
       "46       7.214812\n",
       "92       6.445248\n",
       "184  ...</td>\n",
       "      <td>step\n",
       "46       0.001484\n",
       "92       0.001583\n",
       "184  ...</td>\n",
       "      <td>step\n",
       "20       9.301204\n",
       "40       7.688476\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.844221\n",
       "40       0.625263\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>394788864.0</td>\n",
       "      <td>2.062025e+09</td>\n",
       "      <td>2.371879e+09</td>\n",
       "      <td>2.626812e+09</td>\n",
       "      <td>2.082472e+09</td>\n",
       "      <td>1.772618e+09</td>\n",
       "      <td>2.082472e+09</td>\n",
       "      <td>347078656.0</td>\n",
       "      <td>395313152.0</td>\n",
       "      <td>10.0       9.301204\n",
       "30.0       7.688476\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rw</td>\n",
       "      <td>base</td>\n",
       "      <td>short</td>\n",
       "      <td>kaplan</td>\n",
       "      <td>704.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>step\n",
       "54       6.982597\n",
       "107      6.311456\n",
       "214  ...</td>\n",
       "      <td>step\n",
       "54       0.001568\n",
       "107      0.001525\n",
       "214  ...</td>\n",
       "      <td>step\n",
       "20       9.452538\n",
       "40       7.725835\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.499740\n",
       "40       0.359278\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>179503104.0</td>\n",
       "      <td>8.369603e+08</td>\n",
       "      <td>1.049985e+09</td>\n",
       "      <td>1.168102e+09</td>\n",
       "      <td>8.942715e+08</td>\n",
       "      <td>6.812467e+08</td>\n",
       "      <td>8.942715e+08</td>\n",
       "      <td>149045248.0</td>\n",
       "      <td>174997504.0</td>\n",
       "      <td>10.0       9.452538\n",
       "30.0       7.725835\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rw</td>\n",
       "      <td>base</td>\n",
       "      <td>long</td>\n",
       "      <td>kaplan</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>step\n",
       "18      8.982249\n",
       "36      8.261229\n",
       "71     ...</td>\n",
       "      <td>step\n",
       "18      0.000969\n",
       "36      0.001106\n",
       "71     ...</td>\n",
       "      <td>step\n",
       "20      9.816062\n",
       "40      8.545821\n",
       "60     ...</td>\n",
       "      <td>step\n",
       "20      2.452952\n",
       "40      1.816782\n",
       "60     ...</td>\n",
       "      <td>...</td>\n",
       "      <td>969105408.0</td>\n",
       "      <td>5.509693e+09</td>\n",
       "      <td>5.964792e+09</td>\n",
       "      <td>6.452143e+09</td>\n",
       "      <td>5.410357e+09</td>\n",
       "      <td>4.955259e+09</td>\n",
       "      <td>5.410357e+09</td>\n",
       "      <td>901726208.0</td>\n",
       "      <td>994131968.0</td>\n",
       "      <td>10.0      9.816062\n",
       "30.0      8.545821\n",
       "50.0    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>owt2</td>\n",
       "      <td>seed</td>\n",
       "      <td>short</td>\n",
       "      <td>const</td>\n",
       "      <td>480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>step\n",
       "3491     3.578323\n",
       "6982     3.449981\n",
       "13909...</td>\n",
       "      <td>step\n",
       "3491     0.002134\n",
       "6982     0.002143\n",
       "13909...</td>\n",
       "      <td>step\n",
       "20       9.114862\n",
       "40       7.545253\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.645117\n",
       "40       0.569670\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>82575360.0</td>\n",
       "      <td>2.698445e+08</td>\n",
       "      <td>4.150886e+08</td>\n",
       "      <td>4.867551e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>1.990656e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>57384960.0</td>\n",
       "      <td>69181440.0</td>\n",
       "      <td>10.0       9.114862\n",
       "30.0       7.545253\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>owt2</td>\n",
       "      <td>seed</td>\n",
       "      <td>short</td>\n",
       "      <td>const</td>\n",
       "      <td>480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>step\n",
       "3491     3.618997\n",
       "6982     3.463371\n",
       "13909...</td>\n",
       "      <td>step\n",
       "3491     0.002187\n",
       "6982     0.002132\n",
       "13909...</td>\n",
       "      <td>step\n",
       "20       9.093253\n",
       "40       7.576393\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.641214\n",
       "40       0.574205\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>82575360.0</td>\n",
       "      <td>2.698445e+08</td>\n",
       "      <td>4.150886e+08</td>\n",
       "      <td>4.867551e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>1.990656e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>57384960.0</td>\n",
       "      <td>69181440.0</td>\n",
       "      <td>10.0       9.093253\n",
       "30.0       7.576393\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>owt2</td>\n",
       "      <td>seed</td>\n",
       "      <td>short</td>\n",
       "      <td>const</td>\n",
       "      <td>480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>step\n",
       "3491     3.592875\n",
       "6982     3.448364\n",
       "13909...</td>\n",
       "      <td>step\n",
       "3491     0.002187\n",
       "6982     0.002127\n",
       "13909...</td>\n",
       "      <td>step\n",
       "20       9.114104\n",
       "40       7.536318\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.643140\n",
       "40       0.571141\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>82575360.0</td>\n",
       "      <td>2.698445e+08</td>\n",
       "      <td>4.150886e+08</td>\n",
       "      <td>4.867551e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>1.990656e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>57384960.0</td>\n",
       "      <td>69181440.0</td>\n",
       "      <td>10.0       9.114104\n",
       "30.0       7.536318\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>owt2</td>\n",
       "      <td>seed</td>\n",
       "      <td>short</td>\n",
       "      <td>const</td>\n",
       "      <td>480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>step\n",
       "1746     3.784541\n",
       "3491     3.574042\n",
       "5040 ...</td>\n",
       "      <td>step\n",
       "1746     0.002292\n",
       "3491     0.002166\n",
       "5040 ...</td>\n",
       "      <td>step\n",
       "20       9.106876\n",
       "40       7.513043\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.642918\n",
       "40       0.571866\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>82575360.0</td>\n",
       "      <td>2.698445e+08</td>\n",
       "      <td>4.150886e+08</td>\n",
       "      <td>4.867551e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>1.990656e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>57384960.0</td>\n",
       "      <td>69181440.0</td>\n",
       "      <td>10.0       9.106876\n",
       "30.0       7.513043\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>owt2</td>\n",
       "      <td>seed</td>\n",
       "      <td>short</td>\n",
       "      <td>const</td>\n",
       "      <td>480.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>step\n",
       "3491     3.588631\n",
       "6982     3.446768\n",
       "13909...</td>\n",
       "      <td>step\n",
       "3491     0.002202\n",
       "6982     0.002161\n",
       "13909...</td>\n",
       "      <td>step\n",
       "20       9.179517\n",
       "40       7.557418\n",
       "60   ...</td>\n",
       "      <td>step\n",
       "20       0.639948\n",
       "40       0.570177\n",
       "60   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>82575360.0</td>\n",
       "      <td>2.698445e+08</td>\n",
       "      <td>4.150886e+08</td>\n",
       "      <td>4.867551e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>1.990656e+08</td>\n",
       "      <td>3.443098e+08</td>\n",
       "      <td>57384960.0</td>\n",
       "      <td>69181440.0</td>\n",
       "      <td>10.0       9.179517\n",
       "30.0       7.557418\n",
       "50.0  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset hparams warmup   decay   width  depth  \\\n",
       "0        rw    base  short  kaplan  1312.0   26.0   \n",
       "1        rw    base  short  kaplan   832.0   21.0   \n",
       "2        rw    base  short  kaplan  1024.0   23.0   \n",
       "3        rw    base  short  kaplan   704.0   18.0   \n",
       "4        rw    base   long  kaplan  1504.0   30.0   \n",
       "..      ...     ...    ...     ...     ...    ...   \n",
       "982    owt2    seed  short   const   480.0   12.0   \n",
       "983    owt2    seed  short   const   480.0   12.0   \n",
       "984    owt2    seed  short   const   480.0   12.0   \n",
       "985    owt2    seed  short   const   480.0   12.0   \n",
       "986    owt2    seed  short   const   480.0   12.0   \n",
       "\n",
       "                                              val/loss  \\\n",
       "0    step\n",
       "26       8.273299\n",
       "52       7.264460\n",
       "104  ...   \n",
       "1    step\n",
       "36       7.209037\n",
       "72       6.616290\n",
       "144  ...   \n",
       "2    step\n",
       "46       7.214812\n",
       "92       6.445248\n",
       "184  ...   \n",
       "3    step\n",
       "54       6.982597\n",
       "107      6.311456\n",
       "214  ...   \n",
       "4    step\n",
       "18      8.982249\n",
       "36      8.261229\n",
       "71     ...   \n",
       "..                                                 ...   \n",
       "982  step\n",
       "3491     3.578323\n",
       "6982     3.449981\n",
       "13909...   \n",
       "983  step\n",
       "3491     3.618997\n",
       "6982     3.463371\n",
       "13909...   \n",
       "984  step\n",
       "3491     3.592875\n",
       "6982     3.448364\n",
       "13909...   \n",
       "985  step\n",
       "1746     3.784541\n",
       "3491     3.574042\n",
       "5040 ...   \n",
       "986  step\n",
       "3491     3.588631\n",
       "6982     3.446768\n",
       "13909...   \n",
       "\n",
       "                                          val/loss_std  \\\n",
       "0    step\n",
       "26       0.001030\n",
       "52       0.001519\n",
       "104  ...   \n",
       "1    step\n",
       "36       0.001431\n",
       "72       0.001538\n",
       "144  ...   \n",
       "2    step\n",
       "46       0.001484\n",
       "92       0.001583\n",
       "184  ...   \n",
       "3    step\n",
       "54       0.001568\n",
       "107      0.001525\n",
       "214  ...   \n",
       "4    step\n",
       "18      0.000969\n",
       "36      0.001106\n",
       "71     ...   \n",
       "..                                                 ...   \n",
       "982  step\n",
       "3491     0.002134\n",
       "6982     0.002143\n",
       "13909...   \n",
       "983  step\n",
       "3491     0.002187\n",
       "6982     0.002132\n",
       "13909...   \n",
       "984  step\n",
       "3491     0.002187\n",
       "6982     0.002127\n",
       "13909...   \n",
       "985  step\n",
       "1746     0.002292\n",
       "3491     0.002166\n",
       "5040 ...   \n",
       "986  step\n",
       "3491     0.002202\n",
       "6982     0.002161\n",
       "13909...   \n",
       "\n",
       "                                            train/loss  \\\n",
       "0    step\n",
       "20       9.578757\n",
       "40       8.079118\n",
       "60   ...   \n",
       "1    step\n",
       "20       8.892697\n",
       "40       7.436884\n",
       "60   ...   \n",
       "2    step\n",
       "20       9.301204\n",
       "40       7.688476\n",
       "60   ...   \n",
       "3    step\n",
       "20       9.452538\n",
       "40       7.725835\n",
       "60   ...   \n",
       "4    step\n",
       "20      9.816062\n",
       "40      8.545821\n",
       "60     ...   \n",
       "..                                                 ...   \n",
       "982  step\n",
       "20       9.114862\n",
       "40       7.545253\n",
       "60   ...   \n",
       "983  step\n",
       "20       9.093253\n",
       "40       7.576393\n",
       "60   ...   \n",
       "984  step\n",
       "20       9.114104\n",
       "40       7.536318\n",
       "60   ...   \n",
       "985  step\n",
       "20       9.106876\n",
       "40       7.513043\n",
       "60   ...   \n",
       "986  step\n",
       "20       9.179517\n",
       "40       7.557418\n",
       "60   ...   \n",
       "\n",
       "                                      train/batch_time  ...   params_all  \\\n",
       "0    step\n",
       "20       1.779191\n",
       "40       1.361588\n",
       "60   ...  ...  672079872.0   \n",
       "1    step\n",
       "20       0.682528\n",
       "40       0.471070\n",
       "60   ...  ...  260063232.0   \n",
       "2    step\n",
       "20       0.844221\n",
       "40       0.625263\n",
       "60   ...  ...  394788864.0   \n",
       "3    step\n",
       "20       0.499740\n",
       "40       0.359278\n",
       "60   ...  ...  179503104.0   \n",
       "4    step\n",
       "20      2.452952\n",
       "40      1.816782\n",
       "60     ...  ...  969105408.0   \n",
       "..                                                 ...  ...          ...   \n",
       "982  step\n",
       "20       0.645117\n",
       "40       0.569670\n",
       "60   ...  ...   82575360.0   \n",
       "983  step\n",
       "20       0.641214\n",
       "40       0.574205\n",
       "60   ...  ...   82575360.0   \n",
       "984  step\n",
       "20       0.643140\n",
       "40       0.571141\n",
       "60   ...  ...   82575360.0   \n",
       "985  step\n",
       "20       0.642918\n",
       "40       0.571866\n",
       "60   ...  ...   82575360.0   \n",
       "986  step\n",
       "20       0.639948\n",
       "40       0.570177\n",
       "60   ...  ...   82575360.0   \n",
       "\n",
       "    flops_per_token_att_no_embed flops_per_token_att flops_per_token_cc  \\\n",
       "0                   3.693920e+09        4.090921e+09       4.459618e+09   \n",
       "1                   1.288176e+09        1.539932e+09       1.729344e+09   \n",
       "2                   2.062025e+09        2.371879e+09       2.626812e+09   \n",
       "3                   8.369603e+08        1.049985e+09       1.168102e+09   \n",
       "4                   5.509693e+09        5.964792e+09       6.452143e+09   \n",
       "..                           ...                 ...                ...   \n",
       "982                 2.698445e+08        4.150886e+08       4.867551e+08   \n",
       "983                 2.698445e+08        4.150886e+08       4.867551e+08   \n",
       "984                 2.698445e+08        4.150886e+08       4.867551e+08   \n",
       "985                 2.698445e+08        4.150886e+08       4.867551e+08   \n",
       "986                 2.698445e+08        4.150886e+08       4.867551e+08   \n",
       "\n",
       "    flops_per_token_no_att flops_per_token_no_att_no_embed  flops_per_token  \\\n",
       "0             3.671753e+09                    3.274752e+09     3.671753e+09   \n",
       "1             1.325236e+09                    1.073480e+09     1.325236e+09   \n",
       "2             2.082472e+09                    1.772618e+09     2.082472e+09   \n",
       "3             8.942715e+08                    6.812467e+08     8.942715e+08   \n",
       "4             5.410357e+09                    4.955259e+09     5.410357e+09   \n",
       "..                     ...                             ...              ...   \n",
       "982           3.443098e+08                    1.990656e+08     3.443098e+08   \n",
       "983           3.443098e+08                    1.990656e+08     3.443098e+08   \n",
       "984           3.443098e+08                    1.990656e+08     3.443098e+08   \n",
       "985           3.443098e+08                    1.990656e+08     3.443098e+08   \n",
       "986           3.443098e+08                    1.990656e+08     3.443098e+08   \n",
       "\n",
       "          params  eff_params_att  \\\n",
       "0    611958784.0     681820160.0   \n",
       "1    220872704.0     256655360.0   \n",
       "2    347078656.0     395313152.0   \n",
       "3    149045248.0     174997504.0   \n",
       "4    901726208.0     994131968.0   \n",
       "..           ...             ...   \n",
       "982   57384960.0      69181440.0   \n",
       "983   57384960.0      69181440.0   \n",
       "984   57384960.0      69181440.0   \n",
       "985   57384960.0      69181440.0   \n",
       "986   57384960.0      69181440.0   \n",
       "\n",
       "                                   train/loss_smoothed  \n",
       "0    10.0       9.578757\n",
       "30.0       8.079118\n",
       "50.0  ...  \n",
       "1    10.0       8.892697\n",
       "30.0       7.436884\n",
       "50.0  ...  \n",
       "2    10.0       9.301204\n",
       "30.0       7.688476\n",
       "50.0  ...  \n",
       "3    10.0       9.452538\n",
       "30.0       7.725835\n",
       "50.0  ...  \n",
       "4    10.0      9.816062\n",
       "30.0      8.545821\n",
       "50.0    ...  \n",
       "..                                                 ...  \n",
       "982  10.0       9.114862\n",
       "30.0       7.545253\n",
       "50.0  ...  \n",
       "983  10.0       9.093253\n",
       "30.0       7.576393\n",
       "50.0  ...  \n",
       "984  10.0       9.114104\n",
       "30.0       7.536318\n",
       "50.0  ...  \n",
       "985  10.0       9.106876\n",
       "30.0       7.513043\n",
       "50.0  ...  \n",
       "986  10.0       9.179517\n",
       "30.0       7.557418\n",
       "50.0  ...  \n",
       "\n",
       "[975 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', 'hparams', 'warmup', 'decay', 'width', 'depth', 'val/loss',\n",
       "       'val/loss_std', 'train/loss', 'train/batch_time', 'train/data_time',\n",
       "       'train/lr', 'warmup_tokens', 'beta1', 'beta2', 'world_size', 'lr',\n",
       "       'seed', 'seq_len', 'vocab_size', 'grad_clip_norm', 'optimizer',\n",
       "       'precision', 'qk_norm', 'z_loss_coefficient', 'bs', 'independent_wd',\n",
       "       'max_step', 'ckpt_path', 'params_active', 'params_active_precise',\n",
       "       'params_no_embed', 'params_all', 'flops_per_token_att_no_embed',\n",
       "       'flops_per_token_att', 'flops_per_token_cc', 'flops_per_token_no_att',\n",
       "       'flops_per_token_no_att_no_embed', 'flops_per_token', 'params',\n",
       "       'eff_params_att', 'train/loss_smoothed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'log10(tokens)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log10(df['val/loss'].map(lambda x: x.index[-1]) * df.bs * df.seq_len), bins=17)\n",
    "plt.title('Training tokens')\n",
    "plt.xlabel('log10(tokens)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      step\n",
       "6650    3.202427\n",
       "Name: val/loss, dtype: f...\n",
       "1      step\n",
       "18423    3.235371\n",
       "Name: val/loss, dtype: ...\n",
       "2      step\n",
       "11724    3.200902\n",
       "Name: val/loss, dtype: ...\n",
       "3      step\n",
       "27301    3.268755\n",
       "Name: val/loss, dtype: ...\n",
       "4      step\n",
       "4513    3.267379\n",
       "Name: val/loss, dtype: f...\n",
       "                             ...                        \n",
       "982    step\n",
       "40320    3.267111\n",
       "Name: val/loss, dtype: ...\n",
       "983    step\n",
       "45360    3.265761\n",
       "Name: val/loss, dtype: ...\n",
       "984    step\n",
       "45360    3.265857\n",
       "Name: val/loss, dtype: ...\n",
       "985    step\n",
       "40320    3.269968\n",
       "Name: val/loss, dtype: ...\n",
       "986    step\n",
       "45360    3.260507\n",
       "Name: val/loss, dtype: ...\n",
       "Name: val/loss, Length: 975, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['val/loss'].map(lambda x: x.iloc[-2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7586236756795135"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = np.array([1, 2])\n",
    "exps = np.array([1, 2])\n",
    "e = 0\n",
    "data = np.array([np.e, np.e ** 2])\n",
    "log_pred(exps, coefs, e, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3,4]])\n",
    "np.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/resolving-scaling-law-discrepancies/analysis_parametric.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  show_df['t'] = df['val/loss'].map(lambda x: x.index[-1]) * df.bs * df.seq_len\n",
      "/workspaces/resolving-scaling-law-discrepancies/analysis_parametric.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  show_df['final_loss'] = df['val/loss'].map(lambda x: x.iloc[-1])\n",
      "/opt/conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:532: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x0 = np.atleast_1d(np.asarray(x0))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mperform_main_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFIGURE1_CONFIGS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/resolving-scaling-law-discrepancies/analysis_parametric.py:60\u001b[0m, in \u001b[0;36mperform_main_analysis\u001b[0;34m(results_df, configs, seed, seed_noise_args, keep_bs_lr_keys)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# data, optimal_pairs, max_loss, min_loss = interp_flop(\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#     show_df, seed_noise = seed_noise_args[config], \u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#     flop_vals=flop_vals, **ISOFLOP_ARGS[config[-2:]],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# fit_results = fit_compute_optimal_power_laws(optimal_pairs, data, fit_loss=fit_loss)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     init_params \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;241m3\u001b[39m), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m3\u001b[39m), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 60\u001b[0m     fit_results \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuber_loss_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshow_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdepth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m(dataset\u001b[38;5;241m=\u001b[39mdataset, hparams\u001b[38;5;241m=\u001b[39mhparams, warmup\u001b[38;5;241m=\u001b[39mwarmup, decay\u001b[38;5;241m=\u001b[39mdecay, param_count\u001b[38;5;241m=\u001b[39mparam_count, val\u001b[38;5;241m=\u001b[39mval, \n\u001b[1;32m     63\u001b[0m                     fit_results\u001b[38;5;241m=\u001b[39mfit_results,\n\u001b[1;32m     64\u001b[0m                     )) \u001b[38;5;66;03m# optimal_pairs=optimal_pairs, data=data, max_loss=max_loss, min_loss=min_loss,))\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_minimize.py:708\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    706\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 708\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:1372\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m-> 1372\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m   1376\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:114\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    111\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m _x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# promotes to floating\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_dtype \u001b[38;5;241m=\u001b[39m _dtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/scipy/_lib/array_api_compat/common/_aliases.py:235\u001b[0m, in \u001b[0;36mastype\u001b[0;34m(x, dtype, copy)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "summary_df = perform_main_analysis(df, FIGURE1_CONFIGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
